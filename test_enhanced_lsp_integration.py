"""
Test Enhanced Tool Calling Integration with existing LSP v2 system
Demonstrates integration with LSPEnhancedSandbox and diagnostic feedback
"""

import asyncio
import pytest
import logging
import sys
import tempfile
import yaml
from pathlib import Path

# Add the project root to Python path
sys.path.insert(0, str(Path(__file__).parent))

from tool_calling.lsp_integration import LSPIntegratedToolExecutor, create_feedback_formatter
from tool_calling.enhanced_executor import EnhancedToolExecutor

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def test_feedback_formatter():
    """Test the LSP diagnostic feedback formatter"""
    print("\n=== Testing LSP Diagnostic Feedback Formatter ===")
    
    formatter = create_feedback_formatter()
    
    # Test with no errors
    empty_diagnostics = {}
    feedback = formatter(empty_diagnostics, "create_file")
    print(f"No errors: {feedback}")
    assert "0 linter errors" in feedback
    
    # Test with syntax errors
    error_diagnostics = {
        "test.py": [
            {
                "severity": "error",
                "line": 5,
                "character": 10,
                "message": "Syntax error: unexpected token",
                "source": "Python"
            },
            {
                "severity": "warning", 
                "line": 8,
                "character": 0,
                "message": "Unused variable 'x'",
                "source": "Pyflakes"
            }
        ]
    }
    
    feedback = formatter(error_diagnostics, "write_text")
    print(f"With errors: {feedback}")
    assert "1 linter error(s)" in feedback
    assert "ERROR [6:11]" in feedback
    
    # Test with multiple files
    multi_file_diagnostics = {
        "file1.py": [
            {
                "severity": "error",
                "line": 0,
                "character": 0, 
                "message": "Import error",
                "source": "Python"
            }
        ],
        "file2.py": [
            {
                "severity": "error",
                "line": 10,
                "character": 5,
                "message": "Type error",
                "source": "Mypy"
            }
        ]
    }
    
    feedback = formatter(multi_file_diagnostics, "multiedit")
    print(f"Multiple files: {feedback}")
    assert "2 linter error(s)" in feedback

def test_enhanced_executor_configuration():
    """Test configuration of enhanced executor"""
    print("\n=== Testing Enhanced Executor Configuration ===")
    
    # Load LSP enhanced configuration
    config_path = Path(__file__).parent / "implementations" / "profiles" / "gpt5_nano_lsp_enhanced.yaml"
    
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    
    # Create mock sandbox without LSP capabilities
    class MockRegularSandbox:
        def write_text(self, path, content):
            return {"success": True, "path": path}
    
    regular_sandbox = MockRegularSandbox()
    
    # Test with regular sandbox
    executor = LSPIntegratedToolExecutor.create_from_config(regular_sandbox, config)
    
    assert not executor.lsp_enabled
    assert not executor.has_lsp_sandbox
    print("‚úì Regular sandbox properly configured (LSP disabled)")
    
    # Create mock sandbox with LSP capabilities
    class MockLSPSandbox:
        def __init__(self):
            self.lsp_manager = "mock_lsp_manager"
            
        def write_text(self, path, content):
            return {
                "success": True, 
                "path": path,
                "lsp_diagnostics": {
                    path: [
                        {
                            "severity": "error",
                            "line": 1,
                            "character": 0,
                            "message": "Test error",
                            "source": "Test"
                        }
                    ]
                }
            }
    
    lsp_sandbox = MockLSPSandbox()
    
    # Test with LSP sandbox
    executor = LSPIntegratedToolExecutor.create_from_config(lsp_sandbox, config)
    
    assert executor.lsp_enabled
    assert executor.has_lsp_sandbox
    assert executor.format_lsp_feedback
    print("‚úì LSP sandbox properly configured (LSP enabled)")

@pytest.mark.asyncio
async def test_diagnostic_feedback_integration():
    """Test integration of diagnostic feedback in tool execution"""
    print("\n=== Testing Diagnostic Feedback Integration ===")
    
    config_path = Path(__file__).parent / "implementations" / "profiles" / "gpt5_nano_lsp_enhanced.yaml"
    
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    
    # Mock LSP sandbox that returns diagnostics
    class MockLSPSandbox:
        def __init__(self):
            self.lsp_manager = "mock_lsp_manager"
            
        def write_text(self, path, content):
            # Simulate different diagnostic scenarios based on content
            if "syntax_error" in content:
                return {
                    "success": True,
                    "path": path,
                    "lsp_diagnostics": {
                        path: [
                            {
                                "severity": "error",
                                "line": 2,
                                "character": 8,
                                "message": "SyntaxError: invalid syntax",
                                "source": "Python"
                            }
                        ]
                    }
                }
            elif "clean_code" in content:
                return {
                    "success": True,
                    "path": path,
                    "lsp_diagnostics": {}
                }
            else:
                return {
                    "success": True,
                    "path": path,
                    "lsp_diagnostics": {
                        path: [
                            {
                                "severity": "warning",
                                "line": 5,
                                "character": 0,
                                "message": "Unused import",
                                "source": "Pyflakes"
                            }
                        ]
                    }
                }
    
    sandbox = MockLSPSandbox()
    executor = EnhancedToolExecutor(sandbox, config)
    
    # Test with syntax error
    tool_call = {
        "function": "write_text",
        "arguments": {
            "path": "test_error.py",
            "content": "def syntax_error(\n    print('missing parenthesis')"
        }
    }
    
    result = await executor.execute_tool_call(tool_call)
    print(f"Syntax error result keys: {list(result.keys())}")
    print(f"Full result: {result}")
    
    if "lsp_feedback" in result:
        assert "1 linter error(s)" in result["lsp_feedback"]
        print("‚úì LSP feedback properly generated for syntax error")
    else:
        print("! LSP feedback not generated - checking diagnostic flow")
    
    # Test with clean code
    tool_call = {
        "function": "write_text", 
        "arguments": {
            "path": "test_clean.py",
            "content": "def clean_code():\n    return 'hello world'"
        }
    }
    
    result = await executor.execute_tool_call(tool_call)
    print(f"Clean code result: {result.get('lsp_feedback', 'No feedback')}")
    assert "0 linter errors" in result["lsp_feedback"]
    
    # Test with warnings
    tool_call = {
        "function": "write_text",
        "arguments": {
            "path": "test_warning.py", 
            "content": "import os\ndef hello(): pass"
        }
    }
    
    result = await executor.execute_tool_call(tool_call)
    print(f"Warning result: {result.get('lsp_feedback', 'No feedback')}")

def test_config_validation():
    """Test configuration validation and defaults"""
    print("\n=== Testing Configuration Validation ===")
    
    # Test minimal config
    minimal_config = {
        "enhanced_tools": {
            "lsp_integration": {
                "enabled": True
            }
        }
    }
    
    class MockLSPSandbox:
        def __init__(self):
            self.lsp_manager = "mock"
    
    sandbox = MockLSPSandbox()
    executor = LSPIntegratedToolExecutor.create_from_config(sandbox, minimal_config)
    
    assert executor.lsp_enabled
    assert executor.format_lsp_feedback  # Should default to True
    print("‚úì Minimal config properly handled")
    
    # Test empty config
    empty_config = {}
    executor = LSPIntegratedToolExecutor.create_from_config(sandbox, empty_config)
    
    # With LSP sandbox, it should enable automatically
    print(f"Empty config LSP enabled: {executor.lsp_enabled}")
    print("‚úì Empty config properly handled")

async def main():
    """Run all integration tests"""
    print("üîß Testing Enhanced Tool Calling Integration with LSP v2")
    print("=" * 60)
    
    try:
        test_feedback_formatter()
        test_enhanced_executor_configuration()
        await test_diagnostic_feedback_integration()
        test_config_validation()
        
        print("\n‚úÖ All LSP integration tests completed successfully!")
        print("\nIntegration Summary:")
        print("- ‚úì Compatible with existing LSPEnhancedSandbox")
        print("- ‚úì Formats diagnostics for AI model feedback")
        print("- ‚úì Graceful fallback for regular sandboxes")
        print("- ‚úì Configuration-driven behavior")
        print("- ‚úì Ready for production use")
        
    except Exception as e:
        print(f"\n‚ùå Test failed with error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())